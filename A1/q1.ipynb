{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "opdFPDUIUwFr",
    "outputId": "9b51c58d-62c3-46b1-8ca7-d1548d6aac9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "#tf version should be 2.5 or higher\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data\n",
    "(train_images, train_labels), (test_images, test_labels) = \\\n",
    "      keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale model\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape inputs for CNN layers\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(kernel_size = 3, filters = 12, use_bias = False, padding = 'same', input_shape = (28, 28, 1)),\n",
    "    keras.layers.BatchNormalization(center = True, scale = False),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv2D(kernel_size = 6, filters = 24, use_bias = False, padding = 'same', strides = 2),\n",
    "    keras.layers.BatchNormalization(center = True, scale = False),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv2D(kernel_size = 6, filters = 32, use_bias = False, padding = 'same', strides = 2),\n",
    "    keras.layers.BatchNormalization(center = True, scale = False),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(200, use_bias = False),\n",
    "    keras.layers.BatchNormalization(center = True, scale = False),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(60, use_bias = False),\n",
    "    keras.layers.BatchNormalization(center = True, scale = False),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "          loss='sparse_categorical_crossentropy',\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 74s 37ms/step - loss: 0.2473 - accuracy: 0.9358\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 70s 38ms/step - loss: 0.0920 - accuracy: 0.9754\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 63s 33ms/step - loss: 0.0687 - accuracy: 0.9815\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 42s 23ms/step - loss: 0.0587 - accuracy: 0.9838\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.0484 - accuracy: 0.9865\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.0414 - accuracy: 0.9884\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.0361 - accuracy: 0.9898\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 64s 34ms/step - loss: 0.0331 - accuracy: 0.9912\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 70s 37ms/step - loss: 0.0278 - accuracy: 0.9918\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 68s 36ms/step - loss: 0.0270 - accuracy: 0.9923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24986084910>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model\n",
    "model.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 7ms/step - loss: 0.0278 - accuracy: 0.9935\n",
      "\n",
      "Test accuracy: 0.9934999942779541\n"
     ]
    }
   ],
   "source": [
    "#evaluate\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.4225300e-08, 2.0636460e-06, 7.3080173e-06, 6.6429739e-06,\n",
       "        1.7030276e-07, 1.2548448e-08, 1.9275942e-07, 9.9998045e-01,\n",
       "        1.2944292e-07, 3.0281287e-06],\n",
       "       [5.4155200e-09, 3.4371986e-08, 9.9999964e-01, 2.1957625e-07,\n",
       "        1.9203218e-10, 2.3774524e-11, 6.2083359e-08, 4.1023696e-09,\n",
       "        3.1579791e-08, 2.2955962e-09],\n",
       "       [3.3926556e-11, 9.9999988e-01, 2.2266466e-08, 8.2159213e-10,\n",
       "        1.2287871e-08, 4.7262234e-09, 8.6154408e-09, 1.2329279e-07,\n",
       "        1.2001941e-08, 8.5216390e-11],\n",
       "       [9.9999952e-01, 4.6948654e-09, 1.3584091e-07, 6.3453803e-10,\n",
       "        3.9695390e-08, 5.0637112e-09, 9.8634189e-08, 9.6121315e-09,\n",
       "        1.6288914e-07, 9.4575739e-08],\n",
       "       [1.6418786e-09, 1.2814853e-08, 2.8850006e-10, 4.9717066e-12,\n",
       "        9.9999797e-01, 4.9264419e-09, 3.0573709e-08, 1.3531484e-09,\n",
       "        1.2569196e-08, 2.0638631e-06]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Raw predictions\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "# Print our model's predictions\n",
    "print(np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "# Check our predictions against the ground truths\n",
    "print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment:\n",
    "\n",
    "At first, I changed the optimizer of the model from 'SGD' to 'Adam' to improve the test accuracy, then I followed the points that was mentioned in lecture slides to improve the accuracy on the test dataset by adding dropout layers between every dense layers and adding batch normaliztion in the keras layers. I have also added one additional dense layer with 100 units but the test accuracy dropped less than 99%. Furthermore, I tried adding dropout layer of 20% between every dense layers but the test accuracy did not cross 99.25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
